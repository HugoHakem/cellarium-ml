{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cNMF in `cellarium-ml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stephen Fleming\n",
    "\n",
    "Yang Xu\n",
    "\n",
    "2025.01.02\n",
    "\n",
    "The `cellarium-ml` project:\n",
    "\n",
    "https://github.com/cellarium-ai/cellarium-ml\n",
    "\n",
    "The specific implementation of cNMF we are actively working on:\n",
    "\n",
    "https://github.com/cellarium-ai/cellarium-ml/pull/196"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "`cellarium-ml` implements a variety of algorithms in a way that is scalable to hundreds of millions of cells and beyond. This notebook provides a demo run of Cellarium's implementation of consensus NMF (cNMF). The specific algorithm for NMF is based on \"Online learning for matrix factorization and sparse coding\" by Mairal, Bach, Ponce, and Sapiro (JMLR 2010).\n",
    "\n",
    "__Pre-processing__ which can also be done using `cellarium-ml` but is not part of this notebook.\n",
    "\n",
    "0. Compute highly-variable genes.\n",
    "\n",
    "__Running cNMF happens in four stages:__\n",
    "\n",
    "1. The initial fit on selected highly-variable genes.\n",
    "\n",
    "    This amounts to creating a YAML file and then running a single command from the command line:\n",
    "    ```bash\n",
    "    cellarium-ml nmf fit --config config.yaml\n",
    "    ```\n",
    "\n",
    "2. Interactive plotting in the notebook to help determine optimal `k`, `density_threshold`, and `local_neighborhood_size`.\n",
    "\n",
    "    Uses functions that are currently called `update_consensusD()` and `calculate_rec_error()` in this notebook, along with some plotting.\n",
    "\n",
    "3. Computing per-cell factor loadings.\n",
    "\n",
    "    Uses the function currently called `get_embedding()` in this notebook.\n",
    "\n",
    "4. Re-computing the `k` factor definitions using all genes (not just highly-variable genes).\n",
    "\n",
    "    After running get_embedding(), gene loading for full transcriptome would be updated as the same time per-cell factor loadings were calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: You will need to use the `nmf` branch of `cellarium-ml` on github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from string import Template\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "from cellarium.ml.core import CellariumModule, CellariumPipeline\n",
    "from cellarium.ml.data import (\n",
    "    DistributedAnnDataCollection,\n",
    "    IterableDistributedAnnDataCollectionDataset,\n",
    ")\n",
    "from cellarium.ml.models.nmf import (\n",
    "    NonNegativeMatrixFactorization,\n",
    "    calculate_rec_error,\n",
    "    get_embedding,\n",
    "    update_consensusD,\n",
    ")\n",
    "from cellarium.ml.utilities.data import AnnDataField, densify\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.set_figure_params(scanpy=True, dpi=75, dpi_save=75, vector_friendly=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# choose which values of k to use\n",
    "\n",
    "# for experiment, only run k as 10, 11, and 12\n",
    "k_values: list[int] = list(range(8, 13))  # this thing needs to be a python list\n",
    "\n",
    "# choose how many repeats of NMF to run to create a \"consensus\"\n",
    "\n",
    "num_repeats: int = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demo uses a human heart dataset which is hosted in a google bucket. We will first download the dataset to the machine where this notebook is running, and then we will run cNMF.\n",
    "\n",
    "We require 2 files:\n",
    "\n",
    "- The dataset in h5ad format. In this case the entire dataset is a single h5ad file, but `cellarium-ml` can use an arbitrary number of h5ad files.\n",
    "- The highly variable genes in CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# change these values to run on a different dataset:\n",
    "\n",
    "# define file paths\n",
    "\n",
    "dataset_h5ad = \"gs://yx-data/Strati_pbmc_rna.h5ad\"\n",
    "highly_variable_genes_csv = \"gs://yx-data/Strati_pbmc_rna_2000hvg.csv\"\n",
    "\n",
    "# define working directory\n",
    "\n",
    "working_dir = \"./tmp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p $working_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# locate data\n",
    "\n",
    "local_h5ad = os.path.join(working_dir, \"data.h5ad\")\n",
    "local_hvg_csv = os.path.join(working_dir, \"hvg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# localize data\n",
    "\n",
    "!gsutil cp $dataset_h5ad $local_h5ad\n",
    "!gsutil cp $highly_variable_genes_csv $local_hvg_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part contains a hack to make things seem simpler: we provide a template config.yaml file here and we modify that file according to the inputs above.\n",
    "\n",
    "In reality, you would probably create the config.yaml file directly without using the little helpers in this section of the notebook. But this helper makes things smoother for this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the path to the config file\n",
    "\n",
    "config_file_template = \"../examples/cli_workflow/nmf_config_template.yaml\"  # path of the config template file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adata = sc.read_h5ad(local_h5ad)\n",
    "dataset_ncells = adata.n_obs\n",
    "n_genes_total = adata.n_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn((5, 3))\n",
    "a[:, -1] = 0\n",
    "(a.T @ a).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(0, 5, (5, 7)).float()\n",
    "(x.T @ a).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# modification of the yaml config file to point to this data\n",
    "\n",
    "local_config_yaml = os.path.join(working_dir, \"config.yaml\")\n",
    "\n",
    "with open(config_file_template, \"r\") as file:\n",
    "    yaml_text = file.read()\n",
    "\n",
    "substitutions = {\n",
    "    \"highly_variable_genes_csv\": os.path.abspath(local_hvg_csv),\n",
    "    \"dataset_h5ad\": os.path.abspath(local_h5ad),\n",
    "    \"dataset_ncells\": dataset_ncells,\n",
    "    \"k_values\": k_values,\n",
    "    \"num_repeats\": num_repeats,\n",
    "    \"n_genes_total\": n_genes_total,\n",
    "}\n",
    "\n",
    "template = Template(yaml_text)\n",
    "customized_yaml = template.substitute(substitutions)\n",
    "\n",
    "# write the customized YAML to a local file in the working directory\n",
    "with open(local_config_yaml, \"w\") as file:\n",
    "    file.write(customized_yaml)\n",
    "\n",
    "print(f\"Config YAML written to: {local_config_yaml}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# take a look at the config file we end up with: again you could skip the above and write this file manually\n",
    "\n",
    "!cat $local_config_yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run cNMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cellarium-ml nmf fit -c $local_config_yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the trained NMF model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "\n",
    "def get_pretrained_model_as_pipeline(\n",
    "    trained_model: str = \"../\",\n",
    "    transforms: list[torch.nn.Module] = [],\n",
    ") -> CellariumPipeline:\n",
    "    model = CellariumModule.load_from_checkpoint(trained_model).model\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # insert the trained model params\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # construct the pipeline\n",
    "    pipeline = CellariumPipeline(transforms + [model])\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def get_cellarium_dataset_from_h5ad(\n",
    "    h5ad: str,\n",
    "    batch_size: int = 1024,\n",
    "    shard_size: int | None = None,\n",
    "    shuffle: bool = False,\n",
    "    drop_last_indices: bool = False,\n",
    ") -> IterableDistributedAnnDataCollectionDataset:\n",
    "    \"\"\"\n",
    "    Get IterableDistributedAnnDataCollectionDataset from an h5ad file specifier.\n",
    "\n",
    "    Args:\n",
    "        h5ad: h5ad file, allowing brace notation for several files.\n",
    "        batch_size: Batch size.\n",
    "        shard_size: Shard size.\n",
    "        shuffle: Whether to shuffle the dataset.\n",
    "        drop_last_indices: Whether to drop the last incomplete batch.\n",
    "\n",
    "    Returns:\n",
    "        IterableDistributedAnnDataCollectionDataset.\n",
    "    \"\"\"\n",
    "    dadc = DistributedAnnDataCollection(\n",
    "        h5ad,\n",
    "        shard_size=shard_size,\n",
    "        max_cache_size=1,\n",
    "    )\n",
    "\n",
    "    dataset = IterableDistributedAnnDataCollectionDataset(\n",
    "        dadc,\n",
    "        batch_keys={\n",
    "            \"x_ng\": AnnDataField(attr=\"X\", convert_fn=densify),\n",
    "            \"var_names_g\": AnnDataField(attr=\"var_names\"),\n",
    "            \"obs_names_n\": AnnDataField(attr=\"obs_names\"),\n",
    "        },\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last_indices=drop_last_indices,\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load trained module\n",
    "\n",
    "ckpt_file = os.getcwd() + \"/NMF.ckpt\"  # currently this is hard-coded, but we will change this\n",
    "\n",
    "pipeline = get_pretrained_model_as_pipeline(\n",
    "    trained_model=ckpt_file,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get a dataset object in the necessary cellarium-ml format\n",
    "\n",
    "dataset = get_cellarium_dataset_from_h5ad(\n",
    "    os.path.abspath(local_h5ad),\n",
    "    shard_size=adata.n_obs,\n",
    "    shuffle=False,\n",
    "    drop_last_indices=False,\n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute consensus factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as well as \"stability\" scores, a.k.a. silhouette scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate consensus D for all Ks\n",
    "\n",
    "# change these thresholds if desired\n",
    "density_threshold = 0.2\n",
    "local_neighborhood_size = 0.3\n",
    "\n",
    "consensus_stat = update_consensusD(\n",
    "    nmf_model=pipeline[-1],\n",
    "    density_threshold=density_threshold,\n",
    "    local_neighborhood_size=local_neighborhood_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create clustermap plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (optional) get gene loadings (of highly variable genes) for a specific k\n",
    "\n",
    "\n",
    "def get_gene_loadings_for_k(k: int, model=pipeline[-1]) -> torch.Tensor:\n",
    "    D_kg = getattr(model, f\"D_{k}_kg\")\n",
    "    D_kg = D_kg.detach().cpu()\n",
    "    return D_kg\n",
    "\n",
    "\n",
    "D_kg = get_gene_loadings_for_k(10)\n",
    "print(D_kg.shape)\n",
    "# D_kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k in k_values:\n",
    "    cg = sns.clustermap(\n",
    "        consensus_stat[k][\"topk_euc_dist\"].cpu().numpy(),\n",
    "        row_cluster=True,\n",
    "        col_cluster=True,\n",
    "        cbar_pos=(0.05, 0.25, 0.03, 0.15),\n",
    "        xticklabels=False,\n",
    "        yticklabels=False,\n",
    "        rasterized=True,\n",
    "    )\n",
    "    cg.ax_row_dendrogram.set_visible(False)\n",
    "    cg.ax_col_dendrogram.set_visible(False)\n",
    "    # cg.cax.set_visible(False)\n",
    "    cg.cax.set_ylabel(\"Euclidean distance\")\n",
    "    cg.ax_heatmap.set_title(f\"k = {k}\")\n",
    "    plt.show()\n",
    "\n",
    "    sns.histplot(consensus_stat[k][\"local_neigh_dist\"].cpu().numpy())\n",
    "    ymax = plt.gca().get_ylim()[1]\n",
    "    plt.vlines(density_threshold, ymin=0, ymax=ymax, color=\"Red\")\n",
    "    plt.xlabel(f\"Mean distance to {int(num_repeats * local_neighborhood_size)} nearest neigbors\")\n",
    "    plt.ylabel(\"Runs of NMF\")\n",
    "    plt.title(f\"k = {k} local density histogram\")\n",
    "    plt.ylim(0, ymax)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute reconstruction error at each k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we need to calculate the reconstruction error: this takes time\n",
    "\n",
    "rec_errors = calculate_rec_error(dataset, pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the k-selection plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "silhouette_scores = {}\n",
    "for k in k_values:\n",
    "    silhouette_scores[k] = consensus_stat[k][\"stability\"]\n",
    "eval_metrics = pd.DataFrame.from_dict(silhouette_scores, orient=\"index\")\n",
    "eval_metrics.columns = [\"stability\"]\n",
    "eval_metrics[\"rec_error\"] = rec_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot stability and reconstruction error\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(eval_metrics.index, eval_metrics[\"stability\"], \"o-\", color=\"r\")\n",
    "plt.grid(True)\n",
    "plt.ylabel(\"Stability\", color=\"r\")\n",
    "plt.xlabel(\"Number of components: k\")\n",
    "plt.gca().tick_params(axis=\"y\", colors=\"r\")\n",
    "plt.twinx()\n",
    "plt.plot(eval_metrics.index, eval_metrics[\"rec_error\"], \"o-\", color=\"b\")\n",
    "plt.ylabel(\"Reconstruction error\", color=\"b\")\n",
    "plt.gca().tick_params(axis=\"y\", colors=\"b\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cellarium_normalized_data(x_ng: torch.Tensor) -> torch.Tensor:\n",
    "    std_g = torch.std(x_ng, dim=0) + 1e-4\n",
    "    x_ng = x_ng / std_g\n",
    "    x_ng = torch.clamp(x_ng, min=0.0, max=100.0)\n",
    "    return x_ng\n",
    "\n",
    "\n",
    "def compute_residuals(\n",
    "    dataset: IterableDistributedAnnDataCollectionDataset,\n",
    "    cellarium_nmf: NonNegativeMatrixFactorization,\n",
    "    k: int,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    from cellarium.ml.transforms import Filter\n",
    "\n",
    "    cellarium_nmf.get_rec_error = False  # hacking around strange code design\n",
    "    cellarium_nmf.if_get_full_D = False  # hacking around strange code design\n",
    "    cellarium_nmf.the_best_k = k  # hacking around strange code design\n",
    "\n",
    "    cellarium_factors_kg = getattr(cellarium_nmf, f\"D_{k}_kg\")\n",
    "\n",
    "    numerator_residual = 0.0\n",
    "    denominator_data = 0.0\n",
    "\n",
    "    for batch in tqdm(dataset):\n",
    "        x_ng = torch.from_numpy(batch[\"x_ng\"])\n",
    "        var_names_g = batch[\"var_names_g\"]\n",
    "        filter = Filter([str(s) for s in cellarium_nmf.var_names_hvg])\n",
    "        filter_out = filter(x_ng, var_names_g)\n",
    "        x_ng = filter_out[\"x_ng\"]\n",
    "        var_names_g = filter_out[\"var_names_g\"]\n",
    "        cellarium_loadings_nk = cellarium_nmf.predict(x_ng, var_names_g=var_names_g)[\"alpha_nk\"]\n",
    "        assert isinstance(cellarium_loadings_nk, torch.Tensor)\n",
    "\n",
    "        x_transformed_ng = get_cellarium_normalized_data(x_ng)\n",
    "        frobenius_norm_data = torch.norm(x_transformed_ng, \"fro\")\n",
    "        cellarium_reconstruction_ng = torch.matmul(cellarium_loadings_nk, cellarium_factors_kg)\n",
    "        frobenius_norm_cellarium_residual = torch.norm(x_transformed_ng - cellarium_reconstruction_ng, \"fro\")\n",
    "        numerator_residual += frobenius_norm_cellarium_residual\n",
    "        denominator_data += frobenius_norm_data\n",
    "\n",
    "    unexplained_variance_ratio = (numerator_residual**2) / (denominator_data**2)\n",
    "    return unexplained_variance_ratio, numerator_residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unexplained_variance_ratios = {}\n",
    "numerator_residuals = {}\n",
    "\n",
    "for k in k_values:\n",
    "    unexplained_variance_ratio, numerator_residual = compute_residuals(dataset, pipeline[-1], k)\n",
    "    unexplained_variance_ratios[k] = unexplained_variance_ratio\n",
    "    numerator_residuals[k] = numerator_residual\n",
    "    print(f\"k = {k}: {unexplained_variance_ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(numerator_residuals.keys(), numerator_residuals.values(), \"ro:\")\n",
    "plt.show()\n",
    "plt.plot(unexplained_variance_ratios.keys(), unexplained_variance_ratios.values(), \"ro:\")\n",
    "plt.show()\n",
    "plt.plot(eval_metrics.index, eval_metrics[\"rec_error\"], \"o-\", color=\"b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute per-cell loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loadings of each factor, computed for each cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get per-cell factor loadings using the best k: this takes time\n",
    "\n",
    "best_k = 10\n",
    "obsm_key_added = \"X_nmf\"\n",
    "\n",
    "# set if_get_final_gene_loading as True to get gene loading for full transcriptome\n",
    "df = get_embedding(dataset, pipeline, k=best_k, if_get_final_gene_loading=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add this information to the anndata object\n",
    "\n",
    "adata.obsm[\"X_nmf\"] = df.loc[adata.obs_names].values\n",
    "adata.obsm[\"X_nmf\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# see whether the loadings sum to 1 (nearly)\n",
    "\n",
    "adata.obsm[\"X_nmf\"].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the loadings\n",
    "\n",
    "adata.obsm[\"X_nmf\"] = adata.obsm[\"X_nmf\"] / adata.obsm[\"X_nmf\"].sum(axis=1)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obsm[\"X_nmf\"].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP based on cNMF factor loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compute UMAP\n",
    "\n",
    "sc.pp.neighbors(adata, use_rep=\"X_nmf\", n_neighbors=15, metric=\"cosine\")\n",
    "sc.tl.umap(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.pl.embedding(\n",
    "    adata,\n",
    "    basis=\"umap\",\n",
    "    color=[\"cell_type\", \"disease_setting\", \"therapy\", \"analysis_group\", \"sex\", \"donor_id\"],\n",
    "    ncols=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save gene loadings with different k and the final gene loading of full transcriptome to files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment to run code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # save gene loading (hvg) for all K\n",
    "\n",
    "# output_dir = working_dir+'cnmf_output/'\n",
    "# output_prefix = 'NMF'\n",
    "\n",
    "# if os.path.exists(output_dir):\n",
    "#     print(\"output directary already existed\")\n",
    "# else:\n",
    "#     !mkdir $output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# k_range = pipeline[-1].k_range\n",
    "# for k in k_range:\n",
    "#     D_kg = getattr(pipeline[-1], f\"D_{k}_kg\")\n",
    "#     D_kg = D_kg.cpu().numpy()\n",
    "#     gene_loading_file=output_dir+output_prefix+ \".k=%d.gene_loadings.txt\" % (k)\n",
    "#     np.savetxt(gene_loading_file, D_kg, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # save cell loading with the best k\n",
    "# cell_loading_file=output_dir+output_prefix+ \".k=%d.cell_loadings.txt\" % (best_k)\n",
    "# np.savetxt(cell_loading_file, adata.obsm[obsm_key_added], delimiter='\\t')\n",
    "\n",
    "# # save gene loading with the best k (full transcriptome)\n",
    "# gene_loading_file=output_dir+output_prefix+ \".k=%d.full_gene_loadings.txt\" % (best_k)\n",
    "# full_D_kg = getattr(pipeline[-1], f\"full_D_{best_k}_kg\")\n",
    "# full_D_kg = full_D_kg.cpu().numpy()\n",
    "# np.savetxt(gene_loading_file, full_D_kg, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "cellarium",
   "name": "common-cu122.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu122:m121"
  },
  "kernelspec": {
   "display_name": "cellarium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
