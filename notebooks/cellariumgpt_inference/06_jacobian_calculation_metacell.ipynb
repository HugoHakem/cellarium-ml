{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jacobian calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cellarium.ml import CellariumModule, CellariumPipeline\n",
    "\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "############# NOTE ###################\n",
    "DEVICE = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = \"/mnt/cellariumgpt-xfer/mb-ml-dev-vm\"\n",
    "CHECKPOINTS_PATH = \"/mnt/cellariumgpt-xfer/100M_long_run/run_001/lightning_logs/version_1/checkpoints\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load an AnnData Extract\n",
    "\n",
    "We will use it for category mappings ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an AnnData extract\n",
    "adata_path = os.path.join(ROOT_PATH, \"data\", \"extract_0.h5ad\")\n",
    "adata = sc.read_h5ad(adata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_ontology_infos = dict()\n",
    "\n",
    "ref_obs = adata.obs\n",
    "\n",
    "gene_ontology_infos[\"assay_ontology_term_id\"] = dict()\n",
    "gene_ontology_infos[\"assay_ontology_term_id\"][\"names\"] = list(ref_obs['assay_ontology_term_id'].cat.categories)  \n",
    "gene_ontology_infos[\"assay_ontology_term_id\"][\"labels\"] = list(ref_obs['assay_ontology_term_id'].cat.categories) # just because I am lazy\n",
    "\n",
    "gene_ontology_infos[\"suspension_type\"] = dict()\n",
    "gene_ontology_infos[\"suspension_type\"][\"names\"] = list(ref_obs['suspension_type'].cat.categories)  # for uniformity -- this variable does not have an ontology (does it?)\n",
    "gene_ontology_infos[\"suspension_type\"][\"labels\"] = list(ref_obs['suspension_type'].cat.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gene IDs, gene symbols, useful maps\n",
    "model_var_names = np.asarray(adata.var_names)\n",
    "model_var_names_set = set(model_var_names)\n",
    "var_name_to_index_map = {var_name: i for i, var_name in enumerate(model_var_names)}\n",
    "\n",
    "gene_info_tsv_path = os.path.join(ROOT_PATH, \"gene_info\", \"gene_info.tsv\")\n",
    "gene_info_df = pd.read_csv(gene_info_tsv_path, sep=\"\\t\")\n",
    "\n",
    "gene_symbol_to_gene_id_map = dict()\n",
    "for gene_symbol, gene_id in zip(gene_info_df['Gene Symbol'], gene_info_df['ENSEMBL Gene ID']):\n",
    "    if gene_symbol != float('nan'):\n",
    "        gene_symbol_to_gene_id_map[gene_symbol] = gene_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a CellariumGPT checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = os.path.join(CHECKPOINTS_PATH, \"epoch=2-step=252858.ckpt\")\n",
    "gpt_model = CellariumModule.load_from_checkpoint(ckpt_path, map_location=DEVICE)\n",
    "\n",
    "# Inject gene categories\n",
    "gpt_model.model.gene_categories = np.asarray(adata.var_names)\n",
    "\n",
    "# change attention backend to memory efficient\n",
    "gpt_model.model.set_attention_backend('mem_efficient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the ontology infos from the TrainTokenizer, which is the first step of the pipeline\n",
    "metadata_ontology_infos = gpt_model.pipeline[0].ontology_infos\n",
    "print(type(metadata_ontology_infos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cellarium.ml.models.cellarium_gpt import PredictTokenizer\n",
    "\n",
    "# Rewire the pipeline with a PredictTokenizer\n",
    "predict_tokenizer = PredictTokenizer(\n",
    "    max_total_mrna_umis=100_000,\n",
    "    gene_vocab_sizes={\n",
    "        \"assay\": 19,\n",
    "        \"gene_id\": 36601,\n",
    "        \"gene_value\": 2001,\n",
    "        \"suspension_type\": 2,\n",
    "    },\n",
    "    metadata_vocab_sizes={\n",
    "        \"cell_type\": 890,\n",
    "        \"development_stage\": 191,\n",
    "        \"disease\": 350,\n",
    "        \"sex\": 2,\n",
    "        \"tissue\": 822,\n",
    "    },\n",
    "    ontology_infos=metadata_ontology_infos,\n",
    ")\n",
    "\n",
    "gpt_model.pipeline = CellariumPipeline([\n",
    "    predict_tokenizer,\n",
    "    gpt_model.model,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from cellarium.ml.models.cellarium_gpt import CellariumGPT\n",
    "\n",
    "\n",
    "def generate_tokens_from_adata(\n",
    "        adata: sc.AnnData,\n",
    "        gene_ontology_infos: dict[str, dict[str, list[str]]],\n",
    "        metadata_ontology_infos: dict[str, dict[str, list[str]]],\n",
    "        model_gene_categories: list[str],\n",
    "        obs_index: int | list[int] | None,\n",
    "        query_var_index: list[int],\n",
    "        query_total_mrna_umis: float | None,\n",
    "        metadata_prompt_masks_dict: dict[str, bool],\n",
    "        tokenizer: PredictTokenizer,\n",
    "        device: torch.device = torch.device(\"cpu\"),\n",
    "    ) -> tuple[dict, dict]:\n",
    "    \"\"\"\n",
    "\n",
    "    .. note::\n",
    "      All variables in the AnnData are treated as prompts.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # slice the anndata\n",
    "    if isinstance(obs_index, int):\n",
    "        obs_index = [obs_index]\n",
    "\n",
    "    # save obs before slicing\n",
    "    if obs_index is not None:\n",
    "        adata = adata[obs_index]\n",
    "\n",
    "    # generate gene ids and masks\n",
    "    n_cells = len(adata)\n",
    "    adata_var_names = adata.var_names\n",
    "    model_var_name_to_index_map = {var_name: var_index for var_index, var_name in enumerate(model_gene_categories)}\n",
    "    assert all([var_name in model_var_name_to_index_map for var_name in adata_var_names])\n",
    "    prompt_var_index = [model_var_name_to_index_map[var_name] for var_name in adata_var_names]\n",
    "    n_prompt_vars = len(prompt_var_index)\n",
    "    n_query_vars = len(query_var_index)\n",
    "    n_total_vars = n_prompt_vars + n_query_vars\n",
    "    \n",
    "    # gene id\n",
    "    gene_ids_nc = torch.tensor(\n",
    "        prompt_var_index + query_var_index,\n",
    "        dtype=torch.int64, device=device)[None, :].expand(n_cells, n_total_vars)\n",
    "    \n",
    "    # gene prompt mask\n",
    "    gene_prompt_mask_nc = torch.tensor(\n",
    "        [1] * n_prompt_vars + [0] * n_query_vars,\n",
    "        dtype=torch.bool, device=device)[None, :].expand(n_cells, n_total_vars)\n",
    "    \n",
    "    # gene value\n",
    "    try:\n",
    "        prompt_X_ng = np.asarray(adata.X.todense())\n",
    "    except AttributeError:\n",
    "        prompt_X_ng = adata.X\n",
    "    prompt_gene_value_nc = torch.tensor(prompt_X_ng, dtype=torch.float32, device=device)\n",
    "    query_gene_value_nc = torch.zeros(n_cells, n_query_vars, dtype=torch.float32, device=device)\n",
    "    gene_value_nc = torch.cat([prompt_gene_value_nc, query_gene_value_nc], dim=1)\n",
    "\n",
    "    # total mrna umis\n",
    "    prompt_total_mrna_umis_nc = torch.tensor(\n",
    "        adata.obs[\"total_mrna_umis\"].values,\n",
    "        dtype=torch.float32, device=device)[:, None].expand(n_cells, n_prompt_vars)\n",
    "    if query_total_mrna_umis is None:\n",
    "        # the same as prompt\n",
    "        query_total_mrna_umis_nc = torch.tensor(\n",
    "            adata.obs[\"total_mrna_umis\"].values,\n",
    "            dtype=torch.float32, device=device)[:, None].expand(n_cells, n_query_vars)\n",
    "    else:\n",
    "        query_total_mrna_umis_nc = torch.tensor(\n",
    "            [query_total_mrna_umis] * n_cells,\n",
    "            dtype=torch.float32, device=device)[:, None].expand(n_cells, n_query_vars)\n",
    "    total_mrna_umis_nc = torch.cat([prompt_total_mrna_umis_nc, query_total_mrna_umis_nc], dim=1)\n",
    "\n",
    "    # convert assay and suspension_type to codes\n",
    "    assay_nc = torch.tensor(\n",
    "        pd.Categorical(\n",
    "            adata.obs[\"assay_ontology_term_id\"].values,\n",
    "            categories=gene_ontology_infos[\"assay_ontology_term_id\"][\"names\"]).codes,\n",
    "        dtype=torch.int64, device=device)[:, None].expand(n_cells, n_total_vars)\n",
    "    suspension_type_nc = torch.tensor(\n",
    "        pd.Categorical(\n",
    "            adata.obs[\"suspension_type\"].values,\n",
    "            categories=gene_ontology_infos[\"suspension_type\"][\"names\"]).codes,\n",
    "        dtype=torch.int64, device=device)[:, None].expand(n_cells, n_total_vars)\n",
    "\n",
    "    gene_tokens_dict = {\n",
    "        \"assay\": assay_nc,  # categorical\n",
    "        \"suspension_type\": suspension_type_nc,  # categorical\n",
    "        \"gene_id\": gene_ids_nc,  # categorical\n",
    "        \"gene_value\": gene_value_nc,  # continuous\n",
    "        \"total_mrna_umis\": total_mrna_umis_nc,  # continuous\n",
    "    }\n",
    "\n",
    "    # metadata prompt masks\n",
    "    expanded_metadata_prompt_masks_dict = dict()\n",
    "    for key in metadata_ontology_infos.keys():  # note: key order is important ...\n",
    "        expanded_metadata_prompt_masks_dict[key] = torch.tensor(\n",
    "            [metadata_prompt_masks_dict[key]] * n_cells, dtype=torch.bool, device=device)\n",
    "    \n",
    "    # generate metadata tokens dicts; `PredictTokenizer` will convert these to codes\n",
    "    metadata_tokens_dict = {\n",
    "        \"cell_type\": adata.obs[\"cell_type_ontology_term_id\"].values,  # categorical\n",
    "        \"development_stage\": adata.obs[\"development_stage_ontology_term_id\"].values,  # categorical\n",
    "        \"disease\": adata.obs[\"disease_ontology_term_id\"].values,  # categorical\n",
    "        \"sex\": adata.obs[\"sex_ontology_term_id\"].values,  # categorical\n",
    "        \"tissue\": adata.obs[\"tissue_ontology_term_id\"].values,  # categorical\n",
    "    }\n",
    "\n",
    "    # where to find each thing in the context?\n",
    "    context_indices = dict()\n",
    "    context_indices['prompt_genes'] = np.arange(0, n_prompt_vars).tolist()\n",
    "    context_indices['query_genes'] = np.arange(n_prompt_vars, n_query_vars + n_prompt_vars).tolist()\n",
    "    offset = 0\n",
    "    for metadata_key in metadata_ontology_infos.keys():\n",
    "        context_indices[f'query_{metadata_key}'] = n_query_vars + n_prompt_vars + offset\n",
    "        offset += 1\n",
    "\n",
    "    # return gene_tokens_dict, metadata_tokens_dict\n",
    "    tokenizer_output = tokenizer(\n",
    "        metadata_tokens_n=metadata_tokens_dict,\n",
    "        metadata_prompt_masks_n=expanded_metadata_prompt_masks_dict,\n",
    "        gene_tokens_nc=gene_tokens_dict,\n",
    "        gene_prompt_mask_nc=gene_prompt_mask_nc,\n",
    "    )\n",
    "\n",
    "    return tokenizer_output, context_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snap_to_marginal_mean_manifold(\n",
    "        adata: sc.AnnData,\n",
    "        gene_ontology_infos: dict[str, dict[str, list[str]]],\n",
    "        metadata_ontology_infos: dict[str, dict[str, list[str]]],\n",
    "        query_var_names: list[str],\n",
    "        query_total_mrna_umis: float | None,\n",
    "        predict_tokenizer: PredictTokenizer,\n",
    "        cellarium_gpt_module: CellariumModule,\n",
    "        prompt_gene_values_g: torch.Tensor | None = None,\n",
    "    ) -> dict:\n",
    "    \n",
    "    assert len(adata) == 1, \"Only a single cell is allowed\"\n",
    "    \n",
    "    model_gene_categories = cellarium_gpt_module.model.gene_categories\n",
    "    var_name_to_index_map = {var_name: i for i, var_name in enumerate(model_gene_categories)}\n",
    "    query_var_index = [var_name_to_index_map[var_name] for var_name in query_var_names]\n",
    "\n",
    "    metadata_prompt_masks_dict = {\n",
    "        \"cell_type\": False,\n",
    "        \"development_stage\": False,\n",
    "        \"disease\": False,\n",
    "        \"sex\": False,\n",
    "        \"tissue\": False,\n",
    "    }\n",
    "\n",
    "    tokens_dict, context_indices = generate_tokens_from_adata(\n",
    "        adata=adata,\n",
    "        gene_ontology_infos=gene_ontology_infos,\n",
    "        metadata_ontology_infos=metadata_ontology_infos,\n",
    "        model_gene_categories=model_var_names,\n",
    "        obs_index=None,\n",
    "        query_var_index=query_var_index,\n",
    "        query_total_mrna_umis=query_total_mrna_umis,\n",
    "        metadata_prompt_masks_dict=metadata_prompt_masks_dict,\n",
    "        tokenizer=predict_tokenizer,\n",
    "        device=torch.device(\"cpu\"),\n",
    "    )\n",
    "\n",
    "    # convert to cuda\n",
    "    tokens_dict = cellarium_gpt_module.transfer_batch_to_device(tokens_dict, cellarium_gpt_module.device, 0)\n",
    "    \n",
    "    # get a reference to prompt gene values\n",
    "    FIRST_CELL_DIM = 0\n",
    "    GENE_VALUE_DIM = 0\n",
    "    prompt_gene_log1p_values_g = tokens_dict['gene_tokens_nc']['gene_value'][\n",
    "        FIRST_CELL_DIM, context_indices['prompt_genes'], GENE_VALUE_DIM]\n",
    "    \n",
    "    # this is the \"source\"\n",
    "    if prompt_gene_values_g is None:\n",
    "        prompt_gene_values_g = torch.expm1(prompt_gene_log1p_values_g).clone()\n",
    "    \n",
    "    # inject back to tokens_dict to re-establish the reference for Jacobian calculation\n",
    "    tokens_dict['gene_tokens_nc']['gene_value'][\n",
    "        FIRST_CELL_DIM, context_indices['prompt_genes'], GENE_VALUE_DIM] = torch.log1p(prompt_gene_values_g)\n",
    "\n",
    "    # get model predictions\n",
    "    logits_dict = cellarium_gpt_module.model.predict(\n",
    "        gene_tokens_nc=tokens_dict[\"gene_tokens_nc\"],\n",
    "        metadata_tokens_n=tokens_dict[\"metadata_tokens_n\"],\n",
    "        prompt_mask_nc=tokens_dict[\"prompt_mask_nc\"],\n",
    "    )\n",
    "\n",
    "    # note: we use `q` to denote query genes\n",
    "    gene_logits_qk = logits_dict['gene_value'][FIRST_CELL_DIM, context_indices['query_genes'], :]\n",
    "    gene_logits_qk = gene_logits_qk - torch.logsumexp(gene_logits_qk, dim=-1, keepdim=True)\n",
    "    MAX_COUNTS = gene_logits_qk.shape[-1]\n",
    "    log_counts_1_k = torch.arange(0, MAX_COUNTS, device=gene_logits_qk.device).log()\n",
    "    log_counts_2_k = torch.arange(0, MAX_COUNTS, device=gene_logits_qk.device).pow(2).log()\n",
    "    gene_mom_1_q = torch.logsumexp(gene_logits_qk + log_counts_1_k[None, :], dim=-1).exp()\n",
    "    gene_mom_2_q = torch.logsumexp(gene_logits_qk + log_counts_2_k[None, :], dim=-1).exp()\n",
    "    gene_marginal_means_q = gene_mom_1_q\n",
    "    gene_marginal_std_q = torch.clamp(gene_mom_2_q - gene_mom_1_q.pow(2), 0.).sqrt()\n",
    "\n",
    "    return {\n",
    "        'gene_marginal_means_q': gene_marginal_means_q,\n",
    "        'gene_marginal_std_q': gene_marginal_std_q,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jacobian calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PROMPT_GENES_QUERY_PREFIX = 10_000\n",
    "\n",
    "prompt_gene_ids_path = os.path.join(ROOT_PATH, \"data\", \"cellariumgpt_validation\", \"prompt_gene_ids.csv\")\n",
    "query_gene_ids_path = os.path.join(ROOT_PATH, \"data\", \"cellariumgpt_validation\", \"query_gene_ids.csv\")\n",
    "\n",
    "# load prompt and query gene ids\n",
    "prompt_gene_ids = pd.read_csv(prompt_gene_ids_path, header=None).values.flatten().tolist()\n",
    "query_gene_ids = pd.read_csv(query_gene_ids_path, header=None).values.flatten().tolist()\n",
    "\n",
    "if N_PROMPT_GENES_QUERY_PREFIX is not None:\n",
    "    prompt_gene_ids = query_gene_ids[:N_PROMPT_GENES_QUERY_PREFIX]\n",
    "\n",
    "# load a test anndata\n",
    "dataset_root_path = os.path.join(ROOT_PATH, \"data\", \"cellariumgpt_validation\", \"metacells\")\n",
    "\n",
    "# get the list of files\n",
    "dataset_files = sorted([x for x in os.listdir(dataset_root_path) if x.endswith(\".h5ad\")])\n",
    "full_dataset_paths = [os.path.join(dataset_root_path, file) for file in dataset_files]\n",
    "\n",
    "jacobian_point = \"marginal_mean\"\n",
    "query_chunk_size = 500\n",
    "n_workers = 6 ## NOTE\n",
    "max_query_genes = None\n",
    "\n",
    "# generate full output paths\n",
    "full_output_paths = [x.replace(\".h5ad\", f\"__jacobian__{jacobian_point}.pt\") for x in full_dataset_paths]\n",
    "\n",
    "# assigns input and output datasets to the n_workers\n",
    "worker_io_dict = dict()\n",
    "\n",
    "for i in range(n_workers):\n",
    "    worker_io_dict[i] = {\n",
    "        \"input_paths\": full_dataset_paths[i::n_workers],\n",
    "        \"output_paths\": full_output_paths[i::n_workers],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def process_jacobian(input_path, output_path):\n",
    "\n",
    "    test_adata = sc.read_h5ad(input_path)\n",
    "\n",
    "    # make a metacell\n",
    "    X_meta_g = np.asarray(test_adata.X.sum(0))\n",
    "\n",
    "    # set total mrna umis to the mean of the dataset\n",
    "    target_total_mrna_umis = test_adata.obs['total_mrna_umis'].mean()\n",
    "    X_meta_g = X_meta_g * target_total_mrna_umis / X_meta_g.sum()\n",
    "\n",
    "    # make a metacell anndata\n",
    "    adata_meta = test_adata[0, :].copy()\n",
    "    adata_meta.X = X_meta_g\n",
    "    adata_meta.obs['total_mrna_umis'] = [target_total_mrna_umis]\n",
    "\n",
    "    # subset to prompt gene ids\n",
    "    print(f\"Number of prompt genes: {len(prompt_gene_ids)}\")\n",
    "    adata_meta = adata_meta[:, prompt_gene_ids].copy()\n",
    "\n",
    "    # query var names\n",
    "    query_var_names = query_gene_ids\n",
    "    if max_query_genes is not None:\n",
    "        query_var_names = query_var_names[:max_query_genes]\n",
    "    print(f\"Number of query genes: {len(query_var_names)}\")\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        # get the mean marginal\n",
    "        print(\"Calculating marginal mean and std ...\")\n",
    "        prompt_marginal_dict = snap_to_marginal_mean_manifold(\n",
    "            adata=adata_meta,\n",
    "            gene_ontology_infos=gene_ontology_infos,\n",
    "            metadata_ontology_infos=metadata_ontology_infos,\n",
    "            query_var_names=prompt_gene_ids,  #### NOTE\n",
    "            query_total_mrna_umis=None,\n",
    "            predict_tokenizer=predict_tokenizer,\n",
    "            cellarium_gpt_module=gpt_model,\n",
    "        )\n",
    "        query_marginal_dict = snap_to_marginal_mean_manifold(\n",
    "            adata=adata_meta,\n",
    "            gene_ontology_infos=gene_ontology_infos,\n",
    "            metadata_ontology_infos=metadata_ontology_infos,\n",
    "            query_var_names=query_var_names,  #### NOTE\n",
    "            query_total_mrna_umis=None,\n",
    "            predict_tokenizer=predict_tokenizer,\n",
    "            cellarium_gpt_module=gpt_model,\n",
    "        )\n",
    "        print(\"Done.\")\n",
    "\n",
    "    # jacobian point\n",
    "    if jacobian_point == \"actual\":\n",
    "        adata_meta.layers['original'] = adata_meta.X.copy()\n",
    "        print(\"Using the actual metacell counts as the point to calculate the Jacobian on ...\")\n",
    "\n",
    "    elif jacobian_point == \"marginal_mean\":\n",
    "        # inject into the adata\n",
    "        adata_meta.layers['original'] = adata_meta.X.copy()\n",
    "        adata_meta.X = prompt_marginal_dict['gene_marginal_means_q'].detach().cpu().numpy()[None, :]\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    def yield_chunks(lst, n):\n",
    "        for i in range(0, len(lst), n):\n",
    "            yield lst[i:i + n]\n",
    "\n",
    "    query_var_names_chunks = list(yield_chunks(query_var_names, query_chunk_size))\n",
    "    jacobian_chunks = []\n",
    "\n",
    "    print(\"Calculating the Jacobian ...\")\n",
    "    for query_var_names_chunk in tqdm(query_var_names_chunks):\n",
    "\n",
    "        prompt_gene_values_g = torch.tensor(\n",
    "            adata_meta.X[0],\n",
    "            device=gpt_model.device,\n",
    "            dtype=torch.float32)\n",
    "\n",
    "        def _wrapped_snap_to_marginal_mean_manifold(\n",
    "                prompt_gene_values_g: torch.Tensor) -> torch.Tensor:\n",
    "            \n",
    "            return snap_to_marginal_mean_manifold(\n",
    "                adata=adata_meta,\n",
    "                gene_ontology_infos=gene_ontology_infos,\n",
    "                metadata_ontology_infos=metadata_ontology_infos,\n",
    "                query_var_names=query_var_names_chunk,\n",
    "                query_total_mrna_umis=None,\n",
    "                predict_tokenizer=predict_tokenizer,\n",
    "                cellarium_gpt_module=gpt_model,\n",
    "                prompt_gene_values_g=prompt_gene_values_g,\n",
    "            )['gene_marginal_means_q']\n",
    "\n",
    "        chunk_jacobian_qg = torch.autograd.functional.jacobian(\n",
    "            func=_wrapped_snap_to_marginal_mean_manifold, \n",
    "            inputs=prompt_gene_values_g,\n",
    "            create_graph=False,\n",
    "            vectorize=False,\n",
    "        )\n",
    "\n",
    "        jacobian_chunks.append(chunk_jacobian_qg)\n",
    "\n",
    "    jacobian_qg = torch.cat(jacobian_chunks, dim=0)\n",
    "\n",
    "    result_dict = {\n",
    "        'dataset_name': input_path,\n",
    "        'adata_obs': adata_meta.obs,\n",
    "        'jacobian_point': jacobian_point,\n",
    "        'query_var_names': query_var_names,\n",
    "        'prompt_var_names': prompt_gene_ids,\n",
    "        'jacobian_qg': jacobian_qg,\n",
    "        'adata_gene_values_g': np.asarray(adata_meta.layers['original']).flatten(),\n",
    "        'prompt_gene_values_g': prompt_gene_values_g,\n",
    "        'prompt_marginal_dict': prompt_marginal_dict,\n",
    "        'query_marginal_dict': query_marginal_dict,\n",
    "    }\n",
    "\n",
    "    torch.save(result_dict, output_path)\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"TBW\")\n",
    "    parser.add_argument(\"worker_id\", type=int, help=\"Worker ID\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(f\"Worker ID: {args.worker_id}\")\n",
    "    worker_id = int(args.worker_id)\n",
    "    \n",
    "    # get the input and output paths\n",
    "    io_dict = worker_io_dict[worker_id]\n",
    "\n",
    "    # go through the datasets one by one and work on the ones that the output does not exist\n",
    "    for input_path, output_path in tqdm(zip(io_dict[\"input_paths\"], io_dict[\"output_paths\"])):\n",
    "        if os.path.exists(output_path):\n",
    "            print(f\"Output exists: {output_path}, skipping ...\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Working on: {input_path}\")\n",
    "        process_jacobian(input_path, output_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
