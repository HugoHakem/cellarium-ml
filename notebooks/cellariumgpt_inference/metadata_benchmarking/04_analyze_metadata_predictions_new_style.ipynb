{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Analyze metadata predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import typing as t\n",
    "import pickle\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)  # Set the logging level\n",
    "\n",
    "# Create a handler\n",
    "handler = logging.StreamHandler()\n",
    "\n",
    "# Create and set a formatter\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "\n",
    "# Add the handler to the logger\n",
    "logger.addHandler(handler)\n",
    "\n",
    "# To suppress the stupid AnnData warning ...\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Transforming to str index.\")\n",
    "\n",
    "from cellarium.ml.utilities.inference.cellarium_gpt_inference import \\\n",
    "    CellariumGPTInferenceContext\n",
    "from cellarium.ml.utilities.inference.metadata_benchmarking.calculate_metrics import \\\n",
    "    calculate_metrics_for_prediction_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = \"/home/mehrtash/data\"\n",
    "METADATA_PREDICTIONS_ROOT_PATH = os.path.join(ROOT_PATH, \"data\", \"metadata_predictions\")\n",
    "PREFIX_LIST = [\n",
    "    \"10M_001_bs1536\",\n",
    "    \"19M_001_bs2048\",\n",
    "    \"30M_001_bs2560\",\n",
    "    \"100M_long_run_last\"\n",
    "]\n",
    "VAL_ADATA_IDX_RANGE = np.arange(1, 111)\n",
    "N_HOPS_DICT = {\n",
    "    'cell_type': 3,\n",
    "    'development_stage': 3,\n",
    "    'disease': 3,\n",
    "    'tissue': 3,\n",
    "    'sex': 0,\n",
    "}\n",
    "\n",
    "def load_predictions_anndata(val_adata_idx: int, prefix_idx: int) -> sc.AnnData:\n",
    "    path = os.path.join(\n",
    "        METADATA_PREDICTIONS_ROOT_PATH,\n",
    "        PREFIX_LIST[prefix_idx],\n",
    "        f\"extract_{VAL_ADATA_IDX_RANGE[val_adata_idx]}_metadata_prediction_scores.h5ad\")\n",
    "    return sc.read_h5ad(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontology_resource_path = os.path.join(ROOT_PATH, \"data\", \"cellariumgpt_artifacts\", \"ontology\")\n",
    "\n",
    "logger.info(\"Loading ontology resources ...\")\n",
    "ontology_benchmarking_resource_path_dict = {\n",
    "    'cell_type': os.path.join(ontology_resource_path, 'cl_benchmarking_resource.pkl'),\n",
    "    'development_stage': os.path.join(ontology_resource_path, 'hsapdv_benchmarking_resource.pkl'),\n",
    "    'disease': os.path.join(ontology_resource_path, 'mondo_benchmarking_resource.pkl'),\n",
    "    'tissue': os.path.join(ontology_resource_path, 'uberon_benchmarking_resource.pkl'),\n",
    "    'sex': os.path.join(ontology_resource_path, 'sex_benchmarking_resource.pkl'),\n",
    "}\n",
    "\n",
    "ontology_propagation_resource_path_dict = {\n",
    "    'cell_type': os.path.join(ontology_resource_path, 'cl_propagation_resource.pkl'),\n",
    "    'development_stage': os.path.join(ontology_resource_path, 'hsapdv_propagation_resource.pkl'),\n",
    "    'disease': os.path.join(ontology_resource_path, 'mondo_propagation_resource.pkl'),\n",
    "    'tissue': os.path.join(ontology_resource_path, 'uberon_propagation_resource.pkl'),\n",
    "    'sex': os.path.join(ontology_resource_path, 'sex_propagation_resource.pkl'),\n",
    "}\n",
    "\n",
    "# Define number of hops for each metadata key\n",
    "n_hops_dict = {\n",
    "    'cell_type': 3,\n",
    "    'development_stage': 3,\n",
    "    'disease': 3,\n",
    "    'tissue': 3,\n",
    "    'sex': 0,\n",
    "}\n",
    "\n",
    "# Load the benchmarking ontology resources\n",
    "ontology_benchmarking_resource_dicts = {}\n",
    "for meta_key, path in ontology_benchmarking_resource_path_dict.items():\n",
    "    with open(path, \"rb\") as f:\n",
    "        ontology_benchmarking_resource_dicts[meta_key] = pickle.load(f)\n",
    "\n",
    "# Load the propagation ontology resources into a separate dictionary\n",
    "ontology_propagation_resource_dicts = {}\n",
    "for meta_key, path in ontology_propagation_resource_path_dict.items():\n",
    "    with open(path, \"rb\") as f:\n",
    "        ontology_propagation_resource_dicts[meta_key] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_meta_adata_to_query_obj_for_scoring(\n",
    "        meta_adata: sc.AnnData,\n",
    "        metadata_key: str,\n",
    "        scores_col_name_suffix: str = \"class_probs\",\n",
    "        ontology_term_ids_uns_key_name_suffix: str = \"ontology_term_ids\",\n",
    "        ontology_term_id_obs_col_name_suffix: str = \"ontology_term_id\",\n",
    "    ) -> t.Tuple[t.List[dict], t.List[str]]:\n",
    "    scores_col_name = f\"{metadata_key}_{scores_col_name_suffix}\"\n",
    "    ontology_term_ids_uns_key_name = f\"{metadata_key}_{ontology_term_ids_uns_key_name_suffix}\"\n",
    "    ontology_term_id_obs_col_name = f\"{metadata_key}_{ontology_term_id_obs_col_name_suffix}\"\n",
    "    assert scores_col_name in meta_adata.obsm\n",
    "    assert ontology_term_ids_uns_key_name in meta_adata.uns\n",
    "    assert ontology_term_id_obs_col_name in meta_adata.obs.columns\n",
    "    query_objs = []\n",
    "    ground_truth_ontology_term_ids = []\n",
    "    obs_index = meta_adata.obs.index.values\n",
    "    for i_cell in range(len(meta_adata)):\n",
    "        obs_row = meta_adata.obs.iloc[i_cell]\n",
    "        ground_truth_ontology_term_id = obs_row[ontology_term_id_obs_col_name]\n",
    "        query_obj = dict()\n",
    "        query_obj[\"query_cell_id\"] = obs_index[i_cell]\n",
    "        query_obj[\"matches\"] = []\n",
    "        for ontology_term_id, score in zip(\n",
    "                meta_adata.uns[ontology_term_ids_uns_key_name],\n",
    "                meta_adata.obsm[scores_col_name][i_cell, :]):\n",
    "            query_obj[\"matches\"].append({\n",
    "                \"ontology_term_id\": ontology_term_id,\n",
    "                \"score\": score,\n",
    "            })\n",
    "        query_objs.append(query_obj)\n",
    "        ground_truth_ontology_term_ids.append(ground_truth_ontology_term_id)\n",
    "    return query_objs, ground_truth_ontology_term_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "results = defaultdict(list)\n",
    "eps = 1e-8\n",
    "\n",
    "for prefix_idx in tqdm(range(len(PREFIX_LIST))):\n",
    "    prefix = PREFIX_LIST[prefix_idx]\n",
    "    \n",
    "    for val_adata_idx in tqdm(range(len(VAL_ADATA_IDX_RANGE))):\n",
    "        val_adata_id = VAL_ADATA_IDX_RANGE[val_adata_idx]\n",
    "        meta_adata = load_predictions_anndata(val_adata_idx, prefix_idx)\n",
    "\n",
    "        results[\"prefix\"].append(prefix)\n",
    "        results[\"val_adata_id\"].append(val_adata_id)\n",
    "\n",
    "        for key in [\"cell_type\", \"disease\", \"development_stage\", \"tissue\", \"sex\"]:\n",
    "\n",
    "            # Calculate accuracy\n",
    "            query_objs, ground_truth_ontology_term_ids = convert_meta_adata_to_query_obj_for_scoring(\n",
    "                meta_adata=meta_adata,\n",
    "                metadata_key=key)\n",
    "            results_df = calculate_metrics_for_prediction_output(\n",
    "                ground_truth_ontology_term_ids=ground_truth_ontology_term_ids,\n",
    "                model_predictions=query_objs,\n",
    "                ontology_resource=ontology_benchmarking_resource_dicts[key],\n",
    "                num_hops=n_hops_dict[key],\n",
    "                metric_style=\"hop_k_call\").dropna()\n",
    "            \n",
    "            for n_hops in range(N_HOPS_DICT[key] + 1):\n",
    "                hop_accuracy = results_df[f\"hop_{n_hops}_call\"].mean()\n",
    "                results[f\"{key}_hop_{n_hops}_accuracy\"].append(hop_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coarse tissue for each validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_meta_df = pd.read_csv(\n",
    "    os.path.join(ROOT_PATH, \"data\", \"cellariumgpt_artifacts\", \"cellxgene_validation_donors__third_draft.csv\"))\n",
    "\n",
    "# Stephen's validatdion table\n",
    "# what we have called intestine is actually \"small intestine\"\n",
    "tissue_ontology_term_id_to_coarse_name_map = dict()\n",
    "for row in validation_meta_df['tissue_name_ont_coarsename_coarseont'].values:\n",
    "    split_row = row.strip(\"()\").split(', ')\n",
    "    coarse_name = split_row[2].strip(\"'\")\n",
    "    if coarse_name == \"intestine\":\n",
    "        coarse_name = \"small intestine\"\n",
    "    tissue_ontology_term_id_to_coarse_name_map[split_row[1].strip(\"'\")] = coarse_name\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "val_adata_id_to_coarse_tissue_map = dict()\n",
    "for val_idx in tqdm(range(1, 111)):\n",
    "    \n",
    "    val_adata_path = os.path.join(ROOT_PATH, \"data\", \"cellariumgpt_validation\", f\"extract_{val_idx}.h5ad\")\n",
    "    val_adata = sc.read_h5ad(val_adata_path)\n",
    "\n",
    "    obs_df = val_adata.obs\n",
    "    val_adata_id_to_coarse_tissue_map[val_idx] = obs_df['tissue_ontology_term_id'].map(\n",
    "        tissue_ontology_term_id_to_coarse_name_map).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['coarse_tissue'] = df['val_adata_id'].map(val_adata_id_to_coarse_tissue_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df['coarse_tissue'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = master_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.to_csv(\n",
    "    os.path.join(ROOT_PATH, \"data\", \"metadata_predictions\", \"metadata_prediction_scores.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = pd.read_csv(\n",
    "    os.path.join(ROOT_PATH, \"data\", \"metadata_predictions\", \"metadata_prediction_scores.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df[master_df['prefix'] == PREFIX_LIST[0]]['cell_type_hop_0_accuracy'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "# -------------------------------\n",
    "# User-specified options:\n",
    "# Choose which metric to plot: 'f1', 'sensitivity', or 'specificity'\n",
    "value_to_plot = 'accuracy'\n",
    "# Flag to select summary statistic: True uses mean/std, False uses median/IQR.\n",
    "use_mean = True\n",
    "# df = master_df[master_df['coarse_tissue'] == 'blood'].copy()\n",
    "df = master_df.copy()\n",
    "# -------------------------------\n",
    "\n",
    "# Define metric groups and available hops.\n",
    "# For 'sex', only hop 0 is available.\n",
    "metric_groups = {\n",
    "    'cell_type': [0, 1, 2, 3],\n",
    "    'disease': [0, 1, 2, 3],\n",
    "    'tissue': [0, 1, 2, 3],\n",
    "    'development_stage': [0, 1, 2, 3],\n",
    "    'sex': [0]\n",
    "}\n",
    "\n",
    "# Build the list of metric columns (e.g., \"cell_type_hop_0_f1\")\n",
    "selected_metrics = []\n",
    "for group, hops in metric_groups.items():\n",
    "    for hop in hops:\n",
    "        selected_metrics.append(f\"{group}_hop_{hop}_{value_to_plot}\")\n",
    "\n",
    "# Precompute a color palette for each metric column.\n",
    "palette = {}\n",
    "\n",
    "# For cell_type: use Greens.\n",
    "cell_metrics = [f\"cell_type_hop_{hop}_{value_to_plot}\" for hop in metric_groups['cell_type']]\n",
    "n = len(cell_metrics)\n",
    "for i, m in enumerate(cell_metrics):\n",
    "    palette[m] = cm.get_cmap('Greens')((i + 1) / (n + 1))\n",
    "\n",
    "# For disease: use Reds.\n",
    "disease_metrics = [f\"disease_hop_{hop}_{value_to_plot}\" for hop in metric_groups['disease']]\n",
    "n = len(disease_metrics)\n",
    "for i, m in enumerate(disease_metrics):\n",
    "    palette[m] = cm.get_cmap('Reds')((i + 1) / (n + 1))\n",
    "\n",
    "# For tissue: use Blues.\n",
    "tissue_metrics = [f\"tissue_hop_{hop}_{value_to_plot}\" for hop in metric_groups['tissue']]\n",
    "n = len(tissue_metrics)\n",
    "for i, m in enumerate(tissue_metrics):\n",
    "    palette[m] = cm.get_cmap('Blues')((i + 1) / (n + 1))\n",
    "\n",
    "# For development_stage: use Purples.\n",
    "dev_metrics = [f\"development_stage_hop_{hop}_{value_to_plot}\" for hop in metric_groups['development_stage']]\n",
    "n = len(dev_metrics)\n",
    "for i, m in enumerate(dev_metrics):\n",
    "    palette[m] = cm.get_cmap('Purples')((i + 1) / (n + 1))\n",
    "\n",
    "# For sex: assign a fixed color (e.g., orange).\n",
    "sex_metrics = [f\"sex_hop_{hop}_{value_to_plot}\" for hop in metric_groups['sex']]\n",
    "for m in sex_metrics:\n",
    "    palette[m] = 'orange'\n",
    "\n",
    "# Melt the dataframe into long format for the selected metrics.\n",
    "# Assume your original dataframe is named `df`.\n",
    "df_long = df.melt(\n",
    "    id_vars=['prefix', 'val_adata_id'],\n",
    "    value_vars=selected_metrics,\n",
    "    var_name='metric',\n",
    "    value_name=value_to_plot\n",
    ")\n",
    "# Drop any rows with NaN values for the chosen metric.\n",
    "df_long = df_long.dropna(subset=[value_to_plot])\n",
    "\n",
    "# Determine grid size: one row per metric group, columns = max hops among groups.\n",
    "n_rows = len(metric_groups)\n",
    "max_cols = max(len(hops) for hops in metric_groups.values())\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, max_cols, figsize=(2 * max_cols, 3 * n_rows), sharey=True)\n",
    "\n",
    "# Loop over each metric group (row) and each hop (column).\n",
    "for row_idx, (group, hops) in enumerate(metric_groups.items()):\n",
    "    for col_idx in range(max_cols):\n",
    "        # Turn off extra panels.\n",
    "        if col_idx >= len(hops):\n",
    "            axes[row_idx, col_idx].axis('off')\n",
    "            continue\n",
    "\n",
    "        hop = hops[col_idx]\n",
    "        metric_name = f\"{group}_hop_{hop}_{value_to_plot}\"\n",
    "        ax = axes[row_idx, col_idx]\n",
    "\n",
    "        # Filter data for the current metric and drop any NaNs.\n",
    "        df_metric = df_long[df_long['metric'] == metric_name].dropna(subset=[value_to_plot])\n",
    "\n",
    "        # Compute summary statistics per prefix.\n",
    "        if use_mean:\n",
    "            stats = df_metric.groupby('prefix')[value_to_plot].agg(['mean', 'std']).reindex(PREFIX_LIST)\n",
    "            y_vals = stats['mean']\n",
    "            y_err = stats['std']\n",
    "        else:\n",
    "            stats = df_metric.groupby('prefix').agg(\n",
    "                median=(value_to_plot, np.median),\n",
    "                q1=(value_to_plot, lambda x: x.quantile(0.25)),\n",
    "                q3=(value_to_plot, lambda x: x.quantile(0.75))\n",
    "            ).reindex(PREFIX_LIST)\n",
    "            y_vals = stats['median']\n",
    "            y_err_lower = y_vals - stats['q1']\n",
    "            y_err_upper = stats['q3'] - y_vals\n",
    "            y_err = [y_err_lower.values, y_err_upper.values]\n",
    "\n",
    "        x = np.arange(len(PREFIX_LIST))\n",
    "        # Retrieve the precomputed color (default to gray if not found).\n",
    "        line_color = palette.get(metric_name, 'gray')\n",
    "\n",
    "        # Plot black markers with error bars.\n",
    "        ax.errorbar(\n",
    "            x, y_vals, yerr=y_err,\n",
    "            fmt='o', color='black', ecolor='black',\n",
    "            capsize=4, markersize=6, linestyle='None'\n",
    "        )\n",
    "        # Connect the markers with a line in the precomputed color.\n",
    "        ax.plot(x, y_vals, color=line_color, linestyle='-', linewidth=2)\n",
    "\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(PREFIX_LIST, rotation=90)\n",
    "        ax.set_ylim(0, 1)\n",
    "        # Set the title to show the hop value.\n",
    "        ax.set_title(f\"hop {hop}\")\n",
    "\n",
    "        # Label the y-axis with the metric group on the leftmost panel.\n",
    "        if col_idx == 0:\n",
    "            ax.set_ylabel(f\"{group} ({value_to_plot})\", fontsize=12, fontweight='bold')\n",
    "        # Label the x-axis on the bottom row.\n",
    "        if row_idx == n_rows - 1:\n",
    "            ax.set_xlabel('Prefix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_indices = np.argmax(meta_adata.obsm['cell_type_class_probs'], -1)\n",
    "best_probs = np.max(meta_adata.obsm['cell_type_class_probs'], -1)\n",
    "best_names = meta_adata.uns['cell_type_ontology_term_ids'][best_indices]\n",
    "best_labels = list(map(ontology_propagation_resource_dicts[metadata_key]['ontology_term_id_to_label'].get, best_names))\n",
    "term_id_to_idx_map = {term_id: idx for idx, term_id in enumerate(meta_adata.uns[f'{metadata_key}_ontology_term_ids'])}\n",
    "gt_indices = list(map(term_id_to_idx_map.get, meta_adata.obs[f'{metadata_key}_ontology_term_id'].values))\n",
    "bad_indices = [i for i, idx in enumerate(gt_indices) if idx is None]\n",
    "for i in bad_indices:\n",
    "    gt_indices[i] = 0\n",
    "gt_probs = meta_adata.obsm[f'{metadata_key}_class_probs'][np.arange(len(gt_indices)), gt_indices]\n",
    "gt_probs[bad_indices] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_obs = meta_adata.obs.copy()\n",
    "new_obs['best_label'] = best_labels\n",
    "new_obs['best_prob'] = best_probs\n",
    "new_obs['gt_prob'] = gt_probs\n",
    "new_obs['hop_2_inclusion_score'] = results_df[\"hop_2_inclusion_score\"].values\n",
    "new_obs = new_obs[[metadata_key, 'best_label', 'best_prob', 'gt_prob', 'hop_2_inclusion_score', 'cell_type_hop_2_sensitivity']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_obs['cell_type_hop_2_sensitivity'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all rows in a context manager\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(new_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query_obj in query_objs:\n",
    "    if query_obj[\"query_cell_id\"] == \"26540767\":\n",
    "        break\n",
    "term_id_to_score_map = {match[\"ontology_term_id\"]: match[\"score\"] for match in query_obj[\"matches\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type_label = 'oligodendrocyte precursor cell'\n",
    "cell_type_label_to_term_id = {\n",
    "    label: term_id\n",
    "    for term_id, label in ontology_propagation_resource_dicts[metadata_key]['ontology_term_id_to_label'].items()}\n",
    "cell_type_term_id = cell_type_label_to_term_id[cell_type_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cellarium.ml.utilities.inference.metadata_benchmarking.calculate_metrics import \\\n",
    "    calculate_hop_inclusion_scores\n",
    "\n",
    "calculate_hop_inclusion_scores(query_obj, cell_type_term_id, 3, ontology_benchmarking_resource_dicts[metadata_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_hop in range(4):\n",
    "\n",
    "    nodes = ontology_benchmarking_resource_dicts[metadata_key][cell_type_term_id][f'hop_{i_hop}']['nodes']\n",
    "    ancestors = ontology_benchmarking_resource_dicts[metadata_key][cell_type_term_id][f'hop_{i_hop}']['all_ancestors']\n",
    "    descendants = ontology_benchmarking_resource_dicts[metadata_key][cell_type_term_id][f'hop_{i_hop}']['all_descendants']\n",
    "\n",
    "\n",
    "    print(f\"All nodes (hop {i_hop}):\")\n",
    "    total_score = 0\n",
    "    for node in nodes:\n",
    "        label = ontology_propagation_resource_dicts[metadata_key]['ontology_term_id_to_label'][node]\n",
    "        score = term_id_to_score_map[node] if node in term_id_to_score_map else 0\n",
    "        total_score += score\n",
    "        print(f\" - {label} ({score:.3f})\")\n",
    "    print(f\"Total score: {total_score:.3f}\")\n",
    "    print('')\n",
    "\n",
    "    \n",
    "    print(f\"All ancestors (hop {i_hop}):\")\n",
    "    total_score = 0\n",
    "    for ancestor in ancestors:\n",
    "        label = ontology_propagation_resource_dicts[metadata_key]['ontology_term_id_to_label'][ancestor]\n",
    "        score = term_id_to_score_map[ancestor] if ancestor in term_id_to_score_map else 0\n",
    "        total_score += score\n",
    "        print(f\" - {label} ({score:.3f})\")\n",
    "    print(f\"Total score: {total_score:.3f}\")\n",
    "    print('')\n",
    "\n",
    "    print(f\"All descendants (hop {i_hop}):\")\n",
    "    total_score = 0\n",
    "    for descendant in descendants:\n",
    "        label = ontology_propagation_resource_dicts[metadata_key]['ontology_term_id_to_label'][descendant]\n",
    "        score = term_id_to_score_map[descendant] if descendant in term_id_to_score_map else 0\n",
    "        total_score += score\n",
    "        print(f\" - {label} ({score:.3f})\")\n",
    "    print(f\"Total score: {total_score:.3f}\")\n",
    "    print('')\n",
    "    \n",
    "    print('-----------------------\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
