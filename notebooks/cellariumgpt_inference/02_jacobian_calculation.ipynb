{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jacobian calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cellarium.ml import CellariumModule, CellariumPipeline\n",
    "\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "DEVICE = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = \"/mnt/cellariumgpt-xfer/mb-ml-dev-vm\"\n",
    "CHECKPOINTS_PATH = \"/mnt/cellariumgpt-xfer/100M_long_run/run_001/lightning_logs/version_0/checkpoints\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load an AnnData Extract\n",
    "\n",
    "We will use it for category mappings ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an AnnData extract\n",
    "adata_path = os.path.join(ROOT_PATH, \"data\", \"extract_0.h5ad\")\n",
    "adata = sc.read_h5ad(adata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_ontology_infos = dict()\n",
    "\n",
    "ref_obs = adata.obs\n",
    "\n",
    "gene_ontology_infos[\"assay_ontology_term_id\"] = dict()\n",
    "gene_ontology_infos[\"assay_ontology_term_id\"][\"names\"] = list(ref_obs['assay_ontology_term_id'].cat.categories)  \n",
    "gene_ontology_infos[\"assay_ontology_term_id\"][\"labels\"] = list(ref_obs['assay_ontology_term_id'].cat.categories) # just because I am lazy\n",
    "\n",
    "gene_ontology_infos[\"suspension_type\"] = dict()\n",
    "gene_ontology_infos[\"suspension_type\"][\"names\"] = list(ref_obs['suspension_type'].cat.categories)  # for uniformity -- this variable does not have an ontology (does it?)\n",
    "gene_ontology_infos[\"suspension_type\"][\"labels\"] = list(ref_obs['suspension_type'].cat.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gene IDs, gene symbols, useful maps\n",
    "model_var_names = np.asarray(adata.var_names)\n",
    "model_var_names_set = set(model_var_names)\n",
    "var_name_to_index_map = {var_name: i for i, var_name in enumerate(model_var_names)}\n",
    "\n",
    "gene_info_tsv_path = os.path.join(ROOT_PATH, \"gene_info\", \"gene_info.tsv\")\n",
    "gene_info_df = pd.read_csv(gene_info_tsv_path, sep=\"\\t\")\n",
    "\n",
    "gene_symbol_to_gene_id_map = dict()\n",
    "for gene_symbol, gene_id in zip(gene_info_df['Gene Symbol'], gene_info_df['ENSEMBL Gene ID']):\n",
    "    if gene_symbol != float('nan'):\n",
    "        gene_symbol_to_gene_id_map[gene_symbol] = gene_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a CellariumGPT checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = os.path.join(CHECKPOINTS_PATH, \"epoch=2-step=180000.ckpt\")\n",
    "gpt_model = CellariumModule.load_from_checkpoint(ckpt_path, map_location=DEVICE)\n",
    "\n",
    "# Inject gene categories\n",
    "gpt_model.model.gene_categories = np.asarray(adata.var_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the ontology infos from the TrainTokenizer, which is the first step of the pipeline\n",
    "metadata_ontology_infos = gpt_model.pipeline[0].ontology_infos\n",
    "print(type(metadata_ontology_infos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cellarium.ml.models.cellarium_gpt import PredictTokenizer\n",
    "\n",
    "# Rewire the pipeline with a PredictTokenizer\n",
    "predict_tokenizer = PredictTokenizer(\n",
    "    max_total_mrna_umis=100_000,\n",
    "    gene_vocab_sizes={\n",
    "        \"assay\": 19,\n",
    "        \"gene_id\": 36601,\n",
    "        \"gene_value\": 2001,\n",
    "        \"suspension_type\": 2,\n",
    "    },\n",
    "    metadata_vocab_sizes={\n",
    "        \"cell_type\": 890,\n",
    "        \"development_stage\": 191,\n",
    "        \"disease\": 350,\n",
    "        \"sex\": 2,\n",
    "        \"tissue\": 822,\n",
    "    },\n",
    "    ontology_infos=metadata_ontology_infos,\n",
    ")\n",
    "\n",
    "gpt_model.pipeline = CellariumPipeline([\n",
    "    predict_tokenizer,\n",
    "    gpt_model.model,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from cellarium.ml.models.cellarium_gpt import CellariumGPT\n",
    "\n",
    "\n",
    "def generate_tokens_from_adata(\n",
    "        adata: sc.AnnData,\n",
    "        gene_ontology_infos: dict[str, dict[str, list[str]]],\n",
    "        metadata_ontology_infos: dict[str, dict[str, list[str]]],\n",
    "        model_gene_categories: list[str],\n",
    "        obs_index: int | list[int] | None,\n",
    "        query_var_index: list[int],\n",
    "        query_total_mrna_umis: float | None,\n",
    "        metadata_prompt_masks_dict: dict[str, bool],\n",
    "        tokenizer: PredictTokenizer,\n",
    "        device: torch.device = torch.device(\"cpu\"),\n",
    "    ) -> tuple[dict, dict]:\n",
    "    \"\"\"\n",
    "\n",
    "    .. note::\n",
    "      All variables in the AnnData are treated as prompts.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # slice the anndata\n",
    "    if isinstance(obs_index, int):\n",
    "        obs_index = [obs_index]\n",
    "\n",
    "    # save obs before slicing\n",
    "    if obs_index is not None:\n",
    "        adata = adata[obs_index]\n",
    "\n",
    "    # generate gene ids and masks\n",
    "    n_cells = len(adata)\n",
    "    adata_var_names = adata.var_names\n",
    "    model_var_name_to_index_map = {var_name: var_index for var_index, var_name in enumerate(model_gene_categories)}\n",
    "    assert all([var_name in model_var_name_to_index_map for var_name in adata_var_names])\n",
    "    prompt_var_index = [model_var_name_to_index_map[var_name] for var_name in adata_var_names]\n",
    "    n_prompt_vars = len(prompt_var_index)\n",
    "    n_query_vars = len(query_var_index)\n",
    "    n_total_vars = n_prompt_vars + n_query_vars\n",
    "    \n",
    "    # gene id\n",
    "    gene_ids_nc = torch.tensor(\n",
    "        prompt_var_index + query_var_index,\n",
    "        dtype=torch.int64, device=device)[None, :].expand(n_cells, n_total_vars)\n",
    "    \n",
    "    # gene prompt mask\n",
    "    gene_prompt_mask_nc = torch.tensor(\n",
    "        [1] * n_prompt_vars + [0] * n_query_vars,\n",
    "        dtype=torch.bool, device=device)[None, :].expand(n_cells, n_total_vars)\n",
    "    \n",
    "    # gene value\n",
    "    try:\n",
    "        prompt_X_ng = np.asarray(adata.X.todense())\n",
    "    except AttributeError:\n",
    "        prompt_X_ng = adata.X\n",
    "    prompt_gene_value_nc = torch.tensor(prompt_X_ng, dtype=torch.float32, device=device)\n",
    "    query_gene_value_nc = torch.zeros(n_cells, n_query_vars, dtype=torch.float32, device=device)\n",
    "    gene_value_nc = torch.cat([prompt_gene_value_nc, query_gene_value_nc], dim=1)\n",
    "\n",
    "    # total mrna umis\n",
    "    prompt_total_mrna_umis_nc = torch.tensor(\n",
    "        adata.obs[\"total_mrna_umis\"].values,\n",
    "        dtype=torch.float32, device=device)[:, None].expand(n_cells, n_prompt_vars)\n",
    "    if query_total_mrna_umis is None:\n",
    "        # the same as prompt\n",
    "        query_total_mrna_umis_nc = torch.tensor(\n",
    "            adata.obs[\"total_mrna_umis\"].values,\n",
    "            dtype=torch.float32, device=device)[:, None].expand(n_cells, n_query_vars)\n",
    "    else:\n",
    "        query_total_mrna_umis_nc = torch.tensor(\n",
    "            [query_total_mrna_umis] * n_cells,\n",
    "            dtype=torch.float32, device=device)[:, None].expand(n_cells, n_query_vars)\n",
    "    total_mrna_umis_nc = torch.cat([prompt_total_mrna_umis_nc, query_total_mrna_umis_nc], dim=1)\n",
    "\n",
    "    # convert assay and suspension_type to codes\n",
    "    assay_nc = torch.tensor(\n",
    "        pd.Categorical(\n",
    "            adata.obs[\"assay_ontology_term_id\"].values,\n",
    "            categories=gene_ontology_infos[\"assay_ontology_term_id\"][\"names\"]).codes,\n",
    "        dtype=torch.int64, device=device)[:, None].expand(n_cells, n_total_vars)\n",
    "    suspension_type_nc = torch.tensor(\n",
    "        pd.Categorical(\n",
    "            adata.obs[\"suspension_type\"].values,\n",
    "            categories=gene_ontology_infos[\"suspension_type\"][\"names\"]).codes,\n",
    "        dtype=torch.int64, device=device)[:, None].expand(n_cells, n_total_vars)\n",
    "\n",
    "    gene_tokens_dict = {\n",
    "        \"assay\": assay_nc,  # categorical\n",
    "        \"suspension_type\": suspension_type_nc,  # categorical\n",
    "        \"gene_id\": gene_ids_nc,  # categorical\n",
    "        \"gene_value\": gene_value_nc,  # continuous\n",
    "        \"total_mrna_umis\": total_mrna_umis_nc,  # continuous\n",
    "    }\n",
    "\n",
    "    # metadata prompt masks\n",
    "    expanded_metadata_prompt_masks_dict = dict()\n",
    "    for key in metadata_ontology_infos.keys():  # note: key order is important ...\n",
    "        expanded_metadata_prompt_masks_dict[key] = torch.tensor(\n",
    "            [metadata_prompt_masks_dict[key]] * n_cells, dtype=torch.bool, device=device)\n",
    "    \n",
    "    # generate metadata tokens dicts; `PredictTokenizer` will convert these to codes\n",
    "    metadata_tokens_dict = {\n",
    "        \"cell_type\": adata.obs[\"cell_type_ontology_term_id\"].values,  # categorical\n",
    "        \"development_stage\": adata.obs[\"development_stage_ontology_term_id\"].values,  # categorical\n",
    "        \"disease\": adata.obs[\"disease_ontology_term_id\"].values,  # categorical\n",
    "        \"sex\": adata.obs[\"sex_ontology_term_id\"].values,  # categorical\n",
    "        \"tissue\": adata.obs[\"tissue_ontology_term_id\"].values,  # categorical\n",
    "    }\n",
    "\n",
    "    # where to find each thing in the context?\n",
    "    context_indices = dict()\n",
    "    context_indices['prompt_genes'] = np.arange(0, n_prompt_vars).tolist()\n",
    "    context_indices['query_genes'] = np.arange(n_prompt_vars, n_query_vars + n_prompt_vars).tolist()\n",
    "    offset = 0\n",
    "    for metadata_key in metadata_ontology_infos.keys():\n",
    "        context_indices[f'query_{metadata_key}'] = n_query_vars + n_prompt_vars + offset\n",
    "        offset += 1\n",
    "\n",
    "    # return gene_tokens_dict, metadata_tokens_dict\n",
    "    tokenizer_output = tokenizer(\n",
    "        metadata_tokens_n=metadata_tokens_dict,\n",
    "        metadata_prompt_masks_n=expanded_metadata_prompt_masks_dict,\n",
    "        gene_tokens_nc=gene_tokens_dict,\n",
    "        gene_prompt_mask_nc=gene_prompt_mask_nc,\n",
    "    )\n",
    "\n",
    "    return tokenizer_output, context_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snap_to_marginal_mean_manifold(\n",
    "        adata: sc.AnnData,\n",
    "        gene_ontology_infos: dict[str, dict[str, list[str]]],\n",
    "        metadata_ontology_infos: dict[str, dict[str, list[str]]],\n",
    "        query_var_names: list[str],\n",
    "        query_total_mrna_umis: float | None,\n",
    "        predict_tokenizer: PredictTokenizer,\n",
    "        cellarium_gpt_module: CellariumModule,\n",
    "        prompt_gene_values_g: torch.Tensor | None = None,\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \n",
    "    assert len(adata) == 1, \"Only a single cell is allowed\"\n",
    "    \n",
    "    model_gene_categories = cellarium_gpt_module.model.gene_categories\n",
    "    var_name_to_index_map = {var_name: i for i, var_name in enumerate(model_gene_categories)}\n",
    "    query_var_index = [var_name_to_index_map[var_name] for var_name in query_var_names]\n",
    "\n",
    "    metadata_prompt_masks_dict = {\n",
    "        \"cell_type\": False,\n",
    "        \"development_stage\": False,\n",
    "        \"disease\": False,\n",
    "        \"sex\": False,\n",
    "        \"tissue\": False,\n",
    "    }\n",
    "\n",
    "    tokens_dict, context_indices = generate_tokens_from_adata(\n",
    "        adata=adata,\n",
    "        gene_ontology_infos=gene_ontology_infos,\n",
    "        metadata_ontology_infos=metadata_ontology_infos,\n",
    "        model_gene_categories=model_var_names,\n",
    "        obs_index=None,\n",
    "        query_var_index=query_var_index,\n",
    "        query_total_mrna_umis=query_total_mrna_umis,\n",
    "        metadata_prompt_masks_dict=metadata_prompt_masks_dict,\n",
    "        tokenizer=predict_tokenizer,\n",
    "        device=torch.device(\"cpu\"),\n",
    "    )\n",
    "\n",
    "    # convert to cuda\n",
    "    tokens_dict = cellarium_gpt_module.transfer_batch_to_device(tokens_dict, cellarium_gpt_module.device, 0)\n",
    "    \n",
    "    # get a reference to prompt gene values\n",
    "    FIRST_CELL_DIM = 0\n",
    "    GENE_VALUE_DIM = 0\n",
    "    prompt_gene_log1p_values_g = tokens_dict['gene_tokens_nc']['gene_value'][\n",
    "        FIRST_CELL_DIM, context_indices['prompt_genes'], GENE_VALUE_DIM]\n",
    "    \n",
    "    # this is the \"source\"\n",
    "    if prompt_gene_values_g is None:\n",
    "        prompt_gene_values_g = torch.expm1(prompt_gene_log1p_values_g).clone()\n",
    "    \n",
    "    # inject back to tokens_dict to re-establish the reference for Jacobian calculation\n",
    "    tokens_dict['gene_tokens_nc']['gene_value'][\n",
    "        FIRST_CELL_DIM, context_indices['prompt_genes'], GENE_VALUE_DIM] = torch.log1p(prompt_gene_values_g)\n",
    "\n",
    "    # get model predictions\n",
    "    logits_dict = cellarium_gpt_module.model.predict(\n",
    "        gene_tokens_nc=tokens_dict[\"gene_tokens_nc\"],\n",
    "        metadata_tokens_n=tokens_dict[\"metadata_tokens_n\"],\n",
    "        prompt_mask_nc=tokens_dict[\"prompt_mask_nc\"],\n",
    "    )\n",
    "\n",
    "    # note: we use `q` to denote query genes\n",
    "    gene_logits_qk = logits_dict['gene_value'][FIRST_CELL_DIM, context_indices['query_genes'], :]\n",
    "    gene_logits_qk = gene_logits_qk - torch.logsumexp(gene_logits_qk, dim=-1, keepdim=True)\n",
    "    log_counts_k = torch.arange(0, gene_logits_qk.shape[-1], device=gene_logits_qk.device).log()\n",
    "    gene_marginal_means_q = torch.logsumexp(gene_logits_qk + log_counts_k[None, :], dim=-1).exp()\n",
    "\n",
    "    return prompt_gene_values_g, gene_marginal_means_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jacobian calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a test anndata\n",
    "dataset_names = [\n",
    "    \"luca_CD8_ex_LUAD\",\n",
    "    \"luca_CD8_act_LUAD\",\n",
    "    \"luca_CD8_naive_LUAD\",\n",
    "    \"luca_CD8_em_LUAD\",\n",
    "    \"luca_Treg_LUAD\",\n",
    "    \"luca_CD8_ex_normal\",\n",
    "    \"luca_CD8_act_normal\",\n",
    "    \"luca_CD8_naive_normal\",\n",
    "    \"luca_CD8_em_normal\",\n",
    "    \"luca_Treg_normal\",\n",
    "]\n",
    "\n",
    "jacobian_points = [\n",
    "    \"actual\",\n",
    "    \"marginal_mean\"\n",
    "]\n",
    "\n",
    "target_total_mrna_umis = 2078.0732\n",
    "query_chunk_size = 200\n",
    "max_query_genes = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"Process two integers: jacobian_point_index and dataset_name_index.\")\n",
    "    parser.add_argument(\"jacobian_point_index\", type=int, help=\"Index of the Jacobian point\")\n",
    "    parser.add_argument(\"dataset_name_index\", type=int, help=\"Index of the dataset name\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(f\"Jacobian Point Index: {args.jacobian_point_index}\")\n",
    "    print(f\"Dataset Name Index: {args.dataset_name_index}\")\n",
    "\n",
    "    jacobian_point = jacobian_points[int(args.jacobian_point_index)]\n",
    "    dataset_name = dataset_names[int(args.dataset_name_index)]\n",
    "\n",
    "    test_adata_path = os.path.join(ROOT_PATH, \"cellariumgpt_pd1_lung\", \"output\", f\"{dataset_name}.h5ad\")\n",
    "    test_adata = sc.read_h5ad(test_adata_path)\n",
    "\n",
    "    # add total mNRA umis\n",
    "    test_adata.obs['total_mrna_umis'] = test_adata.X.sum(axis=1)\n",
    "\n",
    "    # make a metacell\n",
    "    X_meta_g = np.asarray(test_adata.X.sum(0))\n",
    "\n",
    "    # set total mrna umis to the mean of the dataset\n",
    "    X_meta_g = X_meta_g * target_total_mrna_umis / X_meta_g.sum()\n",
    "\n",
    "    # make a metacell anndata\n",
    "    adata_meta = test_adata[0, :].copy()\n",
    "    adata_meta.X = X_meta_g\n",
    "    adata_meta.obs['total_mrna_umis'] = [target_total_mrna_umis]\n",
    "\n",
    "    # choose top genes\n",
    "    top_genes_path = os.path.join(ROOT_PATH, \"cellariumgpt_pd1_lung\", \"output\", \"immune_top_5000_genes.csv\")\n",
    "    top_genes_df = pd.read_csv(top_genes_path)\n",
    "    top_gene_ids = top_genes_df['gene_id'].values\n",
    "\n",
    "    # restrict top_gene_ids to those in the model categories\n",
    "    top_gene_ids = [gene_id for gene_id in top_gene_ids if gene_id in model_var_names_set]\n",
    "    prompt_gene_ids = top_gene_ids\n",
    "\n",
    "    adata_meta = adata_meta[:, prompt_gene_ids]\n",
    "\n",
    "    # query var names\n",
    "    query_var_names = prompt_gene_ids\n",
    "    if max_query_genes is not None:\n",
    "        query_var_names = query_var_names[:max_query_genes]\n",
    "\n",
    "    # jacobian point\n",
    "    if jacobian_point == \"actual\":\n",
    "        print(\"Using the actual metacell counts as the point to calculate the Jacobian on ...\")\n",
    "\n",
    "    elif jacobian_point == \"marginal_mean\":\n",
    "        with torch.inference_mode():\n",
    "            # get the mean marginal\n",
    "            print(\"Calculating marginal mean ...\")\n",
    "            prompt_gene_values_g, gene_marginal_means_q = snap_to_marginal_mean_manifold(\n",
    "                adata=adata_meta,\n",
    "                gene_ontology_infos=gene_ontology_infos,\n",
    "                metadata_ontology_infos=metadata_ontology_infos,\n",
    "                query_var_names=prompt_gene_ids,\n",
    "                query_total_mrna_umis=None,\n",
    "                predict_tokenizer=predict_tokenizer,\n",
    "                cellarium_gpt_module=gpt_model,\n",
    "            )\n",
    "\n",
    "            # inject into the adata\n",
    "            adata_meta.X = gene_marginal_means_q.detach().cpu().numpy()[None, :]\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    def yield_chunks(lst, n):\n",
    "        for i in range(0, len(lst), n):\n",
    "            yield lst[i:i + n]\n",
    "\n",
    "    query_var_names_chunks = list(yield_chunks(query_var_names, query_chunk_size))\n",
    "    jacobian_chunks = []\n",
    "\n",
    "    for query_var_names_chunk in tqdm(query_var_names_chunks):\n",
    "\n",
    "        prompt_gene_values_g = torch.tensor(\n",
    "            adata_meta.X[0].toarray().flatten(),\n",
    "            device=gpt_model.device,\n",
    "            dtype=torch.float32)\n",
    "\n",
    "        def _wrapped_snap_to_marginal_mean_manifold(\n",
    "                prompt_gene_values_g: torch.Tensor) -> torch.Tensor:\n",
    "            \n",
    "            return snap_to_marginal_mean_manifold(\n",
    "                adata=adata_meta,\n",
    "                gene_ontology_infos=gene_ontology_infos,\n",
    "                metadata_ontology_infos=metadata_ontology_infos,\n",
    "                query_var_names=query_var_names_chunk,\n",
    "                query_total_mrna_umis=None,\n",
    "                predict_tokenizer=predict_tokenizer,\n",
    "                cellarium_gpt_module=gpt_model,\n",
    "                prompt_gene_values_g=prompt_gene_values_g,\n",
    "            )[1]\n",
    "\n",
    "        chunk_jacobian_qg = torch.autograd.functional.jacobian(\n",
    "            func=_wrapped_snap_to_marginal_mean_manifold, \n",
    "            inputs=prompt_gene_values_g,\n",
    "            create_graph=False,\n",
    "            vectorize=False,\n",
    "        )\n",
    "\n",
    "        jacobian_chunks.append(chunk_jacobian_qg)\n",
    "\n",
    "    jacobian_qg = torch.cat(jacobian_chunks, dim=0)\n",
    "\n",
    "    result_dict = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'jacobian_point': jacobian_point,\n",
    "        'query_var_names': query_var_names,\n",
    "        'prompt_var_names': prompt_gene_ids,\n",
    "        'jacobian_qg': jacobian_qg,\n",
    "        'prompt_gene_values_g': adata_meta.X[0].toarray().flatten(),\n",
    "    }\n",
    "\n",
    "    torch.save(result_dict, f\"./output/jacobian__{dataset_name}__{jacobian_point}.pt\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
