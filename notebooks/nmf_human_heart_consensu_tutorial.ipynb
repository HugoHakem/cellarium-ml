{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "home_dir = '/home/jupyter/'\n",
    "ckpt_file = 'NMF.ckpt'\n",
    "config_file = home_dir+'yaml/nmf_heart_config_default.yaml'\n",
    "data_file = home_dir+'data/human_heart_atlas_hvg_scvi_rec.h5ad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type              | Params\n",
      "-----------------------------------------------\n",
      "0 | pipeline | CellariumPipeline | 1     \n",
      "-----------------------------------------------\n",
      "1         Trainable params\n",
      "0         Non-trainable params\n",
      "1         Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "Epoch 0: 100%|█████████████████████████| 67/67 [01:27<00:00,  0.77it/s, v_num=7]`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█████████████████████████| 67/67 [01:27<00:00,  0.77it/s, v_num=7]\n"
     ]
    }
   ],
   "source": [
    "path = os.getcwd()\n",
    "if ckpt_file in os.listdir(path):\n",
    "    print(\"trained checkpoint saved\")\n",
    "else:\n",
    "    # fit cnmf\n",
    "    !cellarium-ml nmf fit -c $config_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output directary already existed\n"
     ]
    }
   ],
   "source": [
    "output_dir = home_dir+'cnmf_output/'\n",
    "output_prefix = 'NMF'\n",
    "\n",
    "if os.path.exists(output_dir):\n",
    "    print(\"output directary already existed\")\n",
    "else:\n",
    "    !mkdir $output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "parent_dir = os.path.dirname('/home/jupyter/')\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import anndata\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cudf\n",
    "import cuml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cellarium.ml.core import CellariumPipeline, CellariumModule\n",
    "from cellarium.ml.utilities.data import AnnDataField, densify\n",
    "from cellarium.ml.core.datamodule import CellariumAnnDataDataModule\n",
    "from cellarium.ml.data import (\n",
    "    DistributedAnnDataCollection,\n",
    "    IterableDistributedAnnDataCollectionDataset,\n",
    ")\n",
    "\n",
    "from cellarium.ml.models.nmf import calculate_rec_error, get_embeddding, update_consensusD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AutosizedDistributedAnnDataCollection(DistributedAnnDataCollection):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        # I'm being lazy here and doing something real ugly\n",
    "        # I want it to take the shard_size from the first file\n",
    "        try:\n",
    "            # this allows super to find the list of filenames\n",
    "            super().__init__(*args, **kwargs)\n",
    "        except AssertionError:\n",
    "            try:\n",
    "                # this allows super to create the cache\n",
    "                kwargs.pop(\"shard_size\")\n",
    "                kwargs = kwargs | {\"shard_size\": 10000}\n",
    "                super().__init__(*args, **kwargs)\n",
    "            except AssertionError:\n",
    "                pass\n",
    "            # load first file and cache it\n",
    "            adata0 = self.cache[self.filenames[0]] = read_h5ad_file(self.filenames[0])\n",
    "            # pull shard_size from that file\n",
    "            kwargs.pop(\"shard_size\")\n",
    "            kwargs = kwargs | {\"shard_size\": len(adata0)}\n",
    "            # finally initialize for real\n",
    "            super().__init__(*args, **kwargs)\n",
    "\n",
    "def get_pretrained_model_as_pipeline(\n",
    "    trained_model: str = \"../\", \n",
    "    transforms: list[torch.nn.Module] = [],\n",
    "    device: str = \"cuda\",\n",
    ") -> CellariumPipeline:\n",
    "\n",
    "    model = CellariumModule.load_from_checkpoint(trained_model).model\n",
    "\n",
    "    # insert the trained model params\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # construct the pipeline\n",
    "    pipeline = CellariumPipeline(transforms + [model])\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "def get_dataset_from_anndata(\n",
    "    adata: anndata.AnnData | str, \n",
    "    batch_size: int = 2048, \n",
    "    shard_size: int | None = None, \n",
    "    shuffle: bool = False, \n",
    "    shuffle_seed: int = 0, \n",
    "    drop_last_indices: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Get IterableDistributedAnnDataCollectionDataset from an AnnData object or h5ad file specifier.\n",
    "\n",
    "    Args:\n",
    "        adata: AnnData object or h5ad file, allowing brace notation for several files.\n",
    "        batch_size: Batch size.\n",
    "        shard_size: Shard size.\n",
    "        shuffle: Whether to shuffle the dataset.\n",
    "        seed: Random seed.\n",
    "        drop_last: Whether to drop the last incomplete batch.\n",
    "\n",
    "    Returns:\n",
    "        IterableDistributedAnnDataCollectionDataset.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(adata, anndata.AnnData):\n",
    "        tmpfile = tempfile.mkstemp(suffix='.h5ad')\n",
    "        adata.write(tmpfile[1])\n",
    "        file = tmpfile[1]\n",
    "    else:\n",
    "        file = adata\n",
    "\n",
    "    dadc = AutosizedDistributedAnnDataCollection(\n",
    "        file,\n",
    "        shard_size=shard_size,\n",
    "        max_cache_size=1,\n",
    "    )\n",
    "\n",
    "    dataset = IterableDistributedAnnDataCollectionDataset(\n",
    "        dadc,\n",
    "        batch_keys={\n",
    "            \"x_ng\": AnnDataField(attr=\"X\", convert_fn=densify),\n",
    "            \"var_names_g\": AnnDataField(attr=\"var_names\"),\n",
    "        },\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        shuffle_seed=shuffle_seed,\n",
    "        drop_last_indices=drop_last_indices,\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load trained pipeline\n",
    "pipeline = get_pretrained_model_as_pipeline(\n",
    "    trained_model=os.getcwd()+'/'+ckpt_file,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get a dataset object\n",
    "dataset = get_dataset_from_anndata(\n",
    "    data_file, \n",
    "    batch_size=2048,\n",
    "    shard_size=665234,\n",
    "    shuffle=False,\n",
    "    shuffle_seed=0,\n",
    "    drop_last_indices=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate consensus D for all Ks\n",
    "density_threshold = 0.2\n",
    "local_neighborhood_size = 0.3\n",
    "consensus_stat = update_consensusD(pipeline,\n",
    "                                   density_threshold = density_threshold, \n",
    "                                   local_neighborhood_size = local_neighborhood_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save gene loading (hvg) for all K\n",
    "k_range = pipeline[-1].k_range\n",
    "for k in k_range:\n",
    "    D_kg = getattr(pipeline[-1], f\"D_{k}_kg\")\n",
    "    D_kg = D_kg.cpu().numpy()\n",
    "    gene_loading_file=output_dir+output_prefix+ \".k=%d.gene_loadings.txt\" % (k)\n",
    "    np.savetxt(gene_loading_file, D_kg, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in k_range:\n",
    "    print('k='+str(k))\n",
    "    \n",
    "    sc.set_figure_params(scanpy=True, dpi=75, dpi_save=75, vector_friendly=True)\n",
    "    sns.set_style(\"ticks\")\n",
    "    sns.clustermap(consensus_stat[k]['topk_euc_dist'].cpu().numpy())\n",
    "    plt.show()\n",
    "    \n",
    "    sns.histplot(consensus_stat[k]['local_neigh_dist'].cpu().numpy())\n",
    "    ymax = plt.gca().get_ylim()[1]\n",
    "    plt.vlines(density_threshold, ymin=0, ymax=ymax, color='Red')\n",
    "    plt.xlabel('Mean distance to k nearest neigbors')\n",
    "    plt.ylim(0, ymax)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate reconstruction error\n",
    "rec_errors = calculate_rec_error(dataset, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "silhouette_scores = {}\n",
    "for k in k_range:\n",
    "    silhouette_scores[k] = consensus_stat[k]['stability']\n",
    "eval_metrics = pd.DataFrame.from_dict(silhouette_scores, orient='index')\n",
    "eval_metrics.columns = ['stability']\n",
    "eval_metrics['rec_error'] = rec_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot stability and reconstruction error\n",
    "sc.set_figure_params(scanpy=True, dpi=75, dpi_save=75, vector_friendly=True, fontsize=10)\n",
    "sns.set_style(\"ticks\")\n",
    "plt.plot(eval_metrics.index, eval_metrics['stability'], \n",
    "         color='Red', label='Stabibility')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.twinx()\n",
    "plt.plot(eval_metrics.index, eval_metrics['rec_error'], \n",
    "         color='Blue', label='Reconstruction error')\n",
    "plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get cell loading with best K\n",
    "the_best_k = 20\n",
    "obsm_key_added = 'X_nmf'\n",
    "adata = get_embeddding(dataset, pipeline, \n",
    "                       the_best_k=the_best_k, obsm_key_added=obsm_key_added)\n",
    "\n",
    "# save cell loading with the best k\n",
    "cell_loading_file=output_dir+output_prefix+ \".k=%d.cell_loadings.txt\" % (the_best_k)\n",
    "np.savetxt(cell_loading_file, adata.obsm[obsm_key_added], delimiter='\\t')\n",
    "\n",
    "# save gene loading with the best k (full transcriptome)\n",
    "gene_loading_file=output_dir+output_prefix+ \".k=%d.full_gene_loadings.txt\" % (the_best_k)\n",
    "full_D_kg = getattr(pipeline[-1], f\"full_D_{the_best_k}_kg\")\n",
    "full_D_kg = full_D_kg.cpu().numpy()\n",
    "np.savetxt(gene_loading_file, full_D_kg, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualization with UMAP\n",
    "reducer = cuml.UMAP(\n",
    "    n_neighbors=15,\n",
    "    n_components=2,\n",
    "    n_epochs=500,\n",
    "    min_dist=0.15,\n",
    "    metric='cosine'\n",
    ")\n",
    "embedding = reducer.fit_transform(adata.obsm['X_nmf'])\n",
    "adata.obsm['X_nmf_umap'] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.set_figure_params(scanpy=True, dpi=75, dpi_save=75, vector_friendly=True, fontsize=7.5)\n",
    "sc.pl.embedding(adata, basis='nmf_umap', color=['cell_type', 'cell_state', 'batch'], ncols=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "cellarium",
   "name": "common-cu122.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu122:m121"
  },
  "kernelspec": {
   "display_name": "cellarium (Local)",
   "language": "python",
   "name": "cellarium"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
