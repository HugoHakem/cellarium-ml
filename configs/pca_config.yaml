# lightning.pytorch==2.2.0.post0
seed_everything: true
trainer:
  accelerator: auto
  strategy:
    class_path: lightning.pytorch.strategies.DDPStrategy
    init_args:
      accelerator: null
      parallel_devices: null
      cluster_environment: null
      checkpoint_io: null
      precision_plugin: null
      ddp_comm_state: null
      ddp_comm_hook: null
      ddp_comm_wrapper: null
      model_averaging_period: null
      process_group_backend: null
      timeout: 0:30:00
      start_method: popen
      output_device: null
      dim: 0
      broadcast_buffers: false  # this is needed for incremental PCA to work
      process_group: null
      bucket_cap_mb: 25
      find_unused_parameters: false
      check_reduction: false
      gradient_as_bucket_view: false
      static_graph: false
      delay_all_reduce_named_params: null
      param_to_hook_all_reduce: null
      mixed_precision: null
      device_mesh: null
  devices: 2  # Number of GPUs
  num_nodes: 1
  precision: null
  logger: null
  callbacks: null
  fast_dev_run: false
  max_epochs: 1  # set to 1 for PCA by default
  min_epochs: null
  max_steps: -1
  min_steps: null
  max_time: null
  limit_train_batches: null
  limit_val_batches: null
  limit_test_batches: null
  limit_predict_batches: null
  overfit_batches: 0.0
  val_check_interval: null
  check_val_every_n_epoch: 1
  num_sanity_val_steps: null
  log_every_n_steps: null
  enable_checkpointing: null
  enable_progress_bar: null
  enable_model_summary: null
  accumulate_grad_batches: 1
  gradient_clip_val: null
  gradient_clip_algorithm: null
  deterministic: null
  benchmark: null
  inference_mode: true
  use_distributed_sampler: true
  profiler: null
  detect_anomaly: false
  barebones: false
  plugins: null
  sync_batchnorm: false
  reload_dataloaders_every_n_epochs: 0
  default_root_dir: tutorial/pca  # Path to save logs and checkpoints
model:
  transforms:
    - class_path: cellarium.ml.transforms.NormalizeTotal
      init_args:
        target_count: 10_000
    - cellarium.ml.transforms.Log1p
    - class_path: cellarium.ml.transforms.ZScore
      init_args:
        mean_g:
          !CheckpointLoader
          file_path: tutorial/onepass/lightning_logs/version_2/checkpoints/epoch=0-step=5.ckpt
          attr: model.mean_g
          convert_fn: null
        std_g:
          !CheckpointLoader
          file_path: tutorial/onepass/lightning_logs/version_2/checkpoints/epoch=0-step=5.ckpt
          attr: model.std_g
          convert_fn: null
        var_names_g:
          !CheckpointLoader
          file_path: tutorial/onepass/lightning_logs/version_2/checkpoints/epoch=0-step=5.ckpt
          attr: model.var_names_g
          convert_fn: numpy.ndarray.tolist
  model:
    class_path: cellarium.ml.models.IncrementalPCA
    init_args:
      n_components: 50  # Number of principal components
      svd_lowrank_niter: 2
      perform_mean_correction: true  # Subtract mean from data
  optim_fn: null
  optim_kwargs: null
  scheduler_fn: null
  scheduler_kwargs: null
data:
  dadc:
    class_path: cellarium.ml.data.DistributedAnnDataCollection
    init_args:
      filenames: gs://cellarium-file-system/curriculum/homo_sap_no_cancer/extract_files/extract_{0..9}.h5ad  # Filenames
      limits: null
      shard_size: 10_000  # Lookup from anndata files
      last_shard_size: null  # No need to set if it is the same
      max_cache_size: 2  # 2 is recommended
      cache_size_strictly_enforced: true
      label: null
      keys: null
      index_unique: null
      convert: null
      indices_strict: true
      obs_columns_to_validate:
        - total_mrna_umis  # obs keys in batch_keys
  batch_keys:  # Keys that go into CellariumPipeline
    x_ng:
      attr: X
      convert_fn: cellarium.ml.utilities.data.densify
    var_names_g:
      attr: var_names
    total_mrna_umis_n:  # NormalizeTotal
      attr: obs
      key: total_mrna_umis
  batch_size: 10_000  # Set to desired
  shuffle: false
  seed: 0
  drop_last: false
  test_mode: false
  num_workers: 2  # Depends how often the data is loaded
ckpt_path: null  # For resuming training
